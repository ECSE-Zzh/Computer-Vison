{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**ECSE415_Final_Project**\n",
        "\n",
        "Shiyuan Qiao: 260967649\n",
        "\n",
        "Zhiheng Zhou: 260955157"
      ],
      "metadata": {
        "id": "Ku7c-IedidXQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YmOpeN3Jcrk",
        "outputId": "adf96167-43a9-42c5-cb30-d5b415637e17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Object-tracking-and-counting-using-YOLOV8'...\n",
            "remote: Enumerating objects: 249, done.\u001b[K\n",
            "remote: Counting objects: 100% (51/51), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 249 (delta 27), reused 15 (delta 4), pack-reused 198\u001b[K\n",
            "Receiving objects: 100% (249/249), 489.13 MiB | 14.45 MiB/s, done.\n",
            "Resolving deltas: 100% (77/77), done.\n",
            "Updating files: 100% (83/83), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/mohamedamine99/Object-tracking-and-counting-using-YOLOV8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-369BmpLTgF",
        "outputId": "9824bbc0-584e-4b4a-ee22-eeafb7b363be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yFUa_3AJJqEk",
        "outputId": "6ec7cc9d-53c7-4dbe-c948-e94b9b0ae1e2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/yolo_detect_and_count.py'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "src_path = \"/content/Object-tracking-and-counting-using-YOLOV8/sort.py\"\n",
        "dest_path = \"/content\"\n",
        "\n",
        "# copy file from source to destination\n",
        "shutil.copy(src_path, dest_path)\n",
        "\n",
        "src_path = \"/content/Object-tracking-and-counting-using-YOLOV8/requirements.txt\"\n",
        "shutil.copy(src_path, dest_path)\n",
        "\n",
        "src_path = \"/content/Object-tracking-and-counting-using-YOLOV8/yolo_detect_and_count.py\"\n",
        "shutil.copy(src_path, dest_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89aX3NLQJ0Ga",
        "outputId": "a0590114-ef92-42ea-bbf5-736952b222a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/Object-tracking-and-counting-using-YOLOV8\n",
            "Collecting filterpy (from -r requirements.txt (line 1))\n",
            "  Downloading filterpy-1.4.5.zip (177 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.19.3)\n",
            "Collecting lap (from -r requirements.txt (line 3))\n",
            "  Downloading lap-0.4.0.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from filterpy->-r requirements.txt (line 1)) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from filterpy->-r requirements.txt (line 1)) (1.11.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from filterpy->-r requirements.txt (line 1)) (3.7.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 2)) (3.2.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 2)) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 2)) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 2)) (2023.9.26)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 2)) (1.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 2)) (23.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy->-r requirements.txt (line 1)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy->-r requirements.txt (line 1)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy->-r requirements.txt (line 1)) (4.45.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy->-r requirements.txt (line 1)) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy->-r requirements.txt (line 1)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->filterpy->-r requirements.txt (line 1)) (1.16.0)\n",
            "Building wheels for collected packages: filterpy, lap\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110458 sha256=37883b2c87ea7b22f1d00ff2a168cd3aac06570b22034ee25e93e5cebad6a650\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/0c/ea/218f266af4ad626897562199fbbcba521b8497303200186102\n",
            "  Building wheel for lap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lap: filename=lap-0.4.0-cp310-cp310-linux_x86_64.whl size=1628967 sha256=c17d9a7c27d087b853e1dd79cc2c915b202ec1b9a1eed88859bfb30165145cfb\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/42/2e/9dfe19270eea279d79e84767ff0d7b8082c3bf776cad00e83d\n",
            "Successfully built filterpy lap\n",
            "Installing collected packages: lap, filterpy\n",
            "Successfully installed filterpy-1.4.5 lap-0.4.0\n"
          ]
        }
      ],
      "source": [
        "%cd /content/Object-tracking-and-counting-using-YOLOV8\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65sHVb9oJ0_S"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import cv2\n",
        "# import sort\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UO3o1i7gJ73b",
        "outputId": "51af6e30-1aff-464b-f1d9-e2cf4b58453a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.0.222 ðŸš€ Python-3.10.12 torch-2.1.0+cu118 CPU (Intel Xeon 2.20GHz)\n",
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 27.9/107.7 GB disk)\n"
          ]
        }
      ],
      "source": [
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()\n",
        "from ultralytics import YOLO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XA9KXYJh8gl",
        "outputId": "1bbdb81e-1501-4dd9-acf9-767232614afa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/Object-tracking-and-counting-using-YOLOV8\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8lETfP_f-QR"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "    SORT: A Simple, Online and Realtime Tracker\n",
        "    Copyright (C) 2016-2020 Alex Bewley alex@bewley.ai\n",
        "\n",
        "    This program is free software: you can redistribute it and/or modify\n",
        "    it under the terms of the GNU General Public License as published by\n",
        "    the Free Software Foundation, either version 3 of the License, or\n",
        "    (at your option) any later version.\n",
        "\n",
        "    This program is distributed in the hope that it will be useful,\n",
        "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
        "    GNU General Public License for more details.\n",
        "\n",
        "    You should have received a copy of the GNU General Public License\n",
        "    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
        "\"\"\"\n",
        "from __future__ import print_function\n",
        "#import matplotlib\n",
        "#matplotlib.use('TkAgg')\n",
        "import os\n",
        "import numpy as np\n",
        "#import matplotlib.pyplot as plt\n",
        "#import matplotlib.patches as patches\n",
        "from skimage import io\n",
        "\n",
        "import glob\n",
        "import time\n",
        "import argparse\n",
        "from filterpy.kalman import KalmanFilter\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "\n",
        "def linear_assignment(cost_matrix):\n",
        "  try:\n",
        "    import lap\n",
        "    _, x, y = lap.lapjv(cost_matrix, extend_cost=True)\n",
        "    return np.array([[y[i],i] for i in x if i >= 0]) #\n",
        "  except ImportError:\n",
        "    from scipy.optimize import linear_sum_assignment\n",
        "    x, y = linear_sum_assignment(cost_matrix)\n",
        "    return np.array(list(zip(x, y)))\n",
        "\n",
        "\n",
        "def iou_batch(bb_test, bb_gt):\n",
        "  \"\"\"\n",
        "  From SORT: Computes IOU between two bboxes in the form [x1,y1,x2,y2]\n",
        "  \"\"\"\n",
        "  bb_gt = np.expand_dims(bb_gt, 0)\n",
        "  bb_test = np.expand_dims(bb_test, 1)\n",
        "\n",
        "  xx1 = np.maximum(bb_test[..., 0], bb_gt[..., 0])\n",
        "  yy1 = np.maximum(bb_test[..., 1], bb_gt[..., 1])\n",
        "  xx2 = np.minimum(bb_test[..., 2], bb_gt[..., 2])\n",
        "  yy2 = np.minimum(bb_test[..., 3], bb_gt[..., 3])\n",
        "  w = np.maximum(0., xx2 - xx1)\n",
        "  h = np.maximum(0., yy2 - yy1)\n",
        "  wh = w * h\n",
        "  o = wh / ((bb_test[..., 2] - bb_test[..., 0]) * (bb_test[..., 3] - bb_test[..., 1])\n",
        "    + (bb_gt[..., 2] - bb_gt[..., 0]) * (bb_gt[..., 3] - bb_gt[..., 1]) - wh)\n",
        "  return(o)\n",
        "\n",
        "\n",
        "def convert_bbox_to_z(bbox):\n",
        "  \"\"\"\n",
        "  Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n",
        "    [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n",
        "    the aspect ratio\n",
        "  \"\"\"\n",
        "  w = bbox[2] - bbox[0]\n",
        "  h = bbox[3] - bbox[1]\n",
        "  x = bbox[0] + w/2.\n",
        "  y = bbox[1] + h/2.\n",
        "  s = w * h    #scale is just area\n",
        "  r = w / float(h)\n",
        "  return np.array([x, y, s, r]).reshape((4, 1))\n",
        "\n",
        "\n",
        "def convert_x_to_bbox(x,score=None):\n",
        "  \"\"\"\n",
        "  Takes a bounding box in the centre form [x,y,s,r] and returns it in the form\n",
        "    [x1,y1,x2,y2] where x1,y1 is the top left and x2,y2 is the bottom right\n",
        "  \"\"\"\n",
        "  w = np.sqrt(x[2] * x[3])\n",
        "  h = x[2] / w\n",
        "  if(score==None):\n",
        "    return np.array([x[0]-w/2.,x[1]-h/2.,x[0]+w/2.,x[1]+h/2.]).reshape((1,4))\n",
        "  else:\n",
        "    return np.array([x[0]-w/2.,x[1]-h/2.,x[0]+w/2.,x[1]+h/2.,score]).reshape((1,5))\n",
        "\n",
        "\n",
        "class KalmanBoxTracker(object):\n",
        "  \"\"\"\n",
        "  This class represents the internal state of individual tracked objects observed as bbox.\n",
        "  \"\"\"\n",
        "  count = 0\n",
        "  def __init__(self,bbox):\n",
        "    \"\"\"\n",
        "    Initialises a tracker using initial bounding box.\n",
        "    \"\"\"\n",
        "    #define constant velocity model\n",
        "    self.kf = KalmanFilter(dim_x=7, dim_z=4)\n",
        "    self.kf.F = np.array([[1,0,0,0,1,0,0],[0,1,0,0,0,1,0],[0,0,1,0,0,0,1],[0,0,0,1,0,0,0],  [0,0,0,0,1,0,0],[0,0,0,0,0,1,0],[0,0,0,0,0,0,1]])\n",
        "    self.kf.H = np.array([[1,0,0,0,0,0,0],[0,1,0,0,0,0,0],[0,0,1,0,0,0,0],[0,0,0,1,0,0,0]])\n",
        "\n",
        "    self.kf.R[2:,2:] *= 10.\n",
        "    self.kf.P[4:,4:] *= 1000. #give high uncertainty to the unobservable initial velocities\n",
        "    self.kf.P *= 10.\n",
        "    self.kf.Q[-1,-1] *= 0.01\n",
        "    self.kf.Q[4:,4:] *= 0.01\n",
        "\n",
        "    self.kf.x[:4] = convert_bbox_to_z(bbox)\n",
        "    self.time_since_update = 0\n",
        "    self.id = KalmanBoxTracker.count\n",
        "    KalmanBoxTracker.count += 1\n",
        "    self.history = []\n",
        "    self.hits = 0\n",
        "    self.hit_streak = 0\n",
        "    self.age = 0\n",
        "\n",
        "  def update(self,bbox):\n",
        "    \"\"\"\n",
        "    Updates the state vector with observed bbox.\n",
        "    \"\"\"\n",
        "    self.time_since_update = 0\n",
        "    self.history = []\n",
        "    self.hits += 1\n",
        "    self.hit_streak += 1\n",
        "    self.kf.update(convert_bbox_to_z(bbox))\n",
        "\n",
        "  def predict(self):\n",
        "    \"\"\"\n",
        "    Advances the state vector and returns the predicted bounding box estimate.\n",
        "    \"\"\"\n",
        "    if((self.kf.x[6]+self.kf.x[2])<=0):\n",
        "      self.kf.x[6] *= 0.0\n",
        "    self.kf.predict()\n",
        "    self.age += 1\n",
        "    if(self.time_since_update>0):\n",
        "      self.hit_streak = 0\n",
        "    self.time_since_update += 1\n",
        "    self.history.append(convert_x_to_bbox(self.kf.x))\n",
        "    return self.history[-1]\n",
        "\n",
        "  def get_state(self):\n",
        "    \"\"\"\n",
        "    Returns the current bounding box estimate.\n",
        "    \"\"\"\n",
        "    return convert_x_to_bbox(self.kf.x)\n",
        "\n",
        "\n",
        "def associate_detections_to_trackers(detections,trackers,iou_threshold = 0.3):\n",
        "  \"\"\"\n",
        "  Assigns detections to tracked object (both represented as bounding boxes)\n",
        "\n",
        "  Returns 3 lists of matches, unmatched_detections and unmatched_trackers\n",
        "  \"\"\"\n",
        "  if(len(trackers)==0):\n",
        "    return np.empty((0,2),dtype=int), np.arange(len(detections)), np.empty((0,5),dtype=int)\n",
        "\n",
        "  iou_matrix = iou_batch(detections, trackers)\n",
        "\n",
        "  if min(iou_matrix.shape) > 0:\n",
        "    a = (iou_matrix > iou_threshold).astype(np.int32)\n",
        "    if a.sum(1).max() == 1 and a.sum(0).max() == 1:\n",
        "        matched_indices = np.stack(np.where(a), axis=1)\n",
        "    else:\n",
        "      matched_indices = linear_assignment(-iou_matrix)\n",
        "  else:\n",
        "    matched_indices = np.empty(shape=(0,2))\n",
        "\n",
        "  unmatched_detections = []\n",
        "  for d, det in enumerate(detections):\n",
        "    if(d not in matched_indices[:,0]):\n",
        "      unmatched_detections.append(d)\n",
        "  unmatched_trackers = []\n",
        "  for t, trk in enumerate(trackers):\n",
        "    if(t not in matched_indices[:,1]):\n",
        "      unmatched_trackers.append(t)\n",
        "\n",
        "  #filter out matched with low IOU\n",
        "  matches = []\n",
        "  for m in matched_indices:\n",
        "    if(iou_matrix[m[0], m[1]]<iou_threshold):\n",
        "      unmatched_detections.append(m[0])\n",
        "      unmatched_trackers.append(m[1])\n",
        "    else:\n",
        "      matches.append(m.reshape(1,2))\n",
        "  if(len(matches)==0):\n",
        "    matches = np.empty((0,2),dtype=int)\n",
        "  else:\n",
        "    matches = np.concatenate(matches,axis=0)\n",
        "\n",
        "  # print(\"matches:\",matches)\n",
        "  # print(\"unmatched detection:\",np.array(unmatched_detections))\n",
        "  # print(\"unmatched trackers:\",np.array(unmatched_trackers))\n",
        "  return matches, np.array(unmatched_detections), np.array(unmatched_trackers)\n",
        "\n",
        "\n",
        "class Sort(object):\n",
        "  def __init__(self, max_age=1, min_hits=1, iou_threshold=0.3):\n",
        "    \"\"\"\n",
        "    Sets key parameters for SORT\n",
        "    \"\"\"\n",
        "    self.max_age = max_age\n",
        "    self.min_hits = min_hits\n",
        "    self.iou_threshold = iou_threshold\n",
        "    self.trackers = []\n",
        "    self.frame_count = 0\n",
        "    self.labels = {}  # Dictionary to store labels associated with each tracker\n",
        "\n",
        "  def update(self, dets=np.empty((0, 6))):\n",
        "    # self.drop_list.append(1)\n",
        "    \"\"\"\n",
        "    Params:\n",
        "      dets - a numpy array of detections in the format [[x1,y1,x2,y2,score],[x1,y1,x2,y2,score],...]\n",
        "    Requires: this method must be called once for each frame even with empty detections (use np.empty((0, 5)) for frames without detections).\n",
        "    Returns the a similar array, where the last column is the object ID.\n",
        "\n",
        "    NOTE: The number of objects returned may differ from the number of detections provided.\n",
        "    \"\"\"\n",
        "    self.frame_count += 1\n",
        "    # get predicted locations from existing trackers.\n",
        "    trks = np.zeros((len(self.trackers), 6))\n",
        "    to_del = []\n",
        "    ret = []\n",
        "    for t, trk in enumerate(trks):\n",
        "      pos = self.trackers[t].predict()[0]\n",
        "      trk[:] = [pos[0], pos[1], pos[2], pos[3], 0, 0]\n",
        "      if np.any(np.isnan(pos)):\n",
        "        to_del.append(t)\n",
        "    trks = np.ma.compress_rows(np.ma.masked_invalid(trks))\n",
        "    for t in reversed(to_del):\n",
        "      self.trackers.pop(t)\n",
        "    matched, unmatched_dets, unmatched_trks = associate_detections_to_trackers(dets,trks, self.iou_threshold)\n",
        "    # print(\"len of dets:\", len(dets))\n",
        "    # print(\"len of matched:\", len(matched))\n",
        "    # print(\"len of unmatched dets:\", len(unmatched_dets))\n",
        "    # print(\"len of unmatched trks:\", len(unmatched_trks))\n",
        "    # update matched trackers with assigned detections\n",
        "    for m in matched:\n",
        "      trk_id = m[1]\n",
        "      self.trackers[m[1]].update(dets[m[0], :])\n",
        "\n",
        "      # Preserve label associated with the tracker---------------------------------------------\n",
        "      label = dets[m[0], 5]  # Assuming label is in the 6th column\n",
        "      self.labels[trk_id] = label\n",
        "\n",
        "    # create and initialise new trackers for unmatched detections\n",
        "    for i in unmatched_dets:\n",
        "        trk = KalmanBoxTracker(dets[i,:])\n",
        "        self.trackers.append(trk)\n",
        "\n",
        "        # Preserve label for the new tracker-----------------------------------------------------\n",
        "        label = dets[i, 5]  # Assuming label is in the 6th column\n",
        "        # self.labels[len(self.trackers) - 1] = label\n",
        "        self.labels[trk.id] = label\n",
        "\n",
        "\n",
        "    i = len(self.trackers)\n",
        "    for trk in reversed(self.trackers):\n",
        "        trk_id = trk.id\n",
        "        # label = self.labels[trk_id]\n",
        "        # d = trk.get_state()[0]\n",
        "        # if (trk.time_since_update < 1) and (trk.hit_streak >= self.min_hits or self.frame_count <= self.min_hits):\n",
        "        #   # ret.append(np.concatenate((d,[trk.id+1])).reshape(1,-1)) # +1 as MOT benchmark requires positive\n",
        "        #   ret.append(np.concatenate((d, [trk.id + 1, label])).reshape(1, -1)) # +1 as MOT benchmark requires positive\n",
        "\n",
        "        #   # print(\"ret shape:\", ret)\n",
        "\n",
        "        # # else:\n",
        "        # #   self.drop_list.pop()\n",
        "        # #   self.drop_list.append(0)\n",
        "        # i -= 1\n",
        "        # # remove dead tracklet\n",
        "        # if(trk.time_since_update > self.max_age):\n",
        "        #   self.trackers.pop(i)\n",
        "        # Check if the trk_id exists in self.labels before accessing it\n",
        "        if trk_id in self.labels:\n",
        "            label = self.labels[trk_id]\n",
        "        else:\n",
        "            self.labels[trk_id] = label\n",
        "            # Handle the case when trk_id doesn't exist in self.labels\n",
        "            label = 99  # Replace \"Default Label\" with your default label value or handle it according to your logic\n",
        "\n",
        "        d = trk.get_state()[0]\n",
        "        if (trk.time_since_update < 1) and (trk.hit_streak >= self.min_hits or self.frame_count <= self.min_hits):\n",
        "            ret.append(np.concatenate((d, [trk.id + 1, label])).reshape(1, -1))\n",
        "        i -= 1\n",
        "        if(trk.time_since_update > self.max_age):\n",
        "            self.trackers.pop(i)\n",
        "    if(len(ret)>0):\n",
        "      return np.concatenate(ret)\n",
        "    return np.empty((0,6))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RdMeyo_FMmf"
      },
      "outputs": [],
      "source": [
        "class YOLOv8_ObjectDetector:\n",
        "    \"\"\"\n",
        "    A class for performing object detection on images and videos using YOLOv8.\n",
        "\n",
        "    Args:\n",
        "    ------------\n",
        "        model_file (str): Path to the YOLOv8 model file or yolo model variant name in ths format: [variant].pt\n",
        "        labels (list[str], optional): A list of class labels for the model. If None, uses the default labels from the model file.\n",
        "        classes (list[int], optional): The classes we want to detect, use it to exclude certain classes from being detected,\n",
        "            by default all classes in labels are detectable.\n",
        "        conf (float, optional): Minimum confidence threshold for object detection.\n",
        "        iou (float, optional): Minimum IOU threshold for non-max suppression.\n",
        "\n",
        "    Attributes:\n",
        "    --------------\n",
        "        classes (list[str]): A list of class labels for the model ( a Dict is also acceptable).\n",
        "        conf (float): Minimum confidence threshold for object detection.\n",
        "        iou (float): Minimum IOU threshold for non-max suppression.\n",
        "        model (YOLO): The YOLOv8 model used for object detection.\n",
        "        model_name (str): The name of the YOLOv8 model file (without the .pt extension).\n",
        "\n",
        "    Methods :\n",
        "    -------------\n",
        "        default_display: Returns a default display (ultralytics plot implementation) of the object detection results.\n",
        "        custom_display: Returns a custom display of the object detection results.\n",
        "        predict_video: Predicts objects in a video and saves the results to a file.\n",
        "        predict_img: Predicts objects in an image and returns the detection results.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_file = 'yolov8n.pt', labels= None, classes = None, conf = 0.25, iou = 0.45 ):\n",
        "\n",
        "        self.classes = classes\n",
        "        self.conf = conf\n",
        "        self.iou = iou\n",
        "\n",
        "        self.model = YOLO(model_file)\n",
        "        self.model_name = model_file.split('.')[0]\n",
        "        self.results = None\n",
        "\n",
        "\n",
        "        # if no labels are provided then use default COCO names\n",
        "        if labels == None:\n",
        "            self.labels = self.model.names\n",
        "        else:\n",
        "            self.labels = labels\n",
        "\n",
        "    def predict_img(self, img, verbose=True):\n",
        "        \"\"\"\n",
        "        Runs object detection on a single image.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "            img (numpy.ndarray): Input image to perform object detection on.\n",
        "            verbose (bool): Whether to print detection details.\n",
        "\n",
        "        Returns:\n",
        "        -----------\n",
        "            'ultralytics.yolo.engine.results.Results': A YOLO results object that contains\n",
        "             details about detection results :\n",
        "                    - Class IDs\n",
        "                    - Bounding Boxes\n",
        "                    - Confidence score\n",
        "                    ...\n",
        "        (pls refer to https://docs.ultralytics.com/reference/results/#results-api-reference for results API reference)\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Run the model on the input image with the given parameters\n",
        "        results = self.model(img, classes=self.classes, conf=self.conf, iou=self.iou, verbose=verbose)\n",
        "\n",
        "        # Save the original image and the results for further analysis if needed\n",
        "        self.orig_img = img\n",
        "        self.results = results[0]\n",
        "\n",
        "        # Return the detection results\n",
        "        return results[0]\n",
        "\n",
        "\n",
        "\n",
        "    def default_display(self, show_conf=True, line_width=None, font_size=None,\n",
        "                        font='Arial.ttf', pil=False, example='abc'):\n",
        "        \"\"\"\n",
        "        Displays the detected objects on the original input image.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        show_conf : bool, optional\n",
        "            Whether to show the confidence score of each detected object, by default True.\n",
        "        line_width : int, optional\n",
        "            The thickness of the bounding box line in pixels, by default None.\n",
        "        font_size : int, optional\n",
        "            The font size of the text label for each detected object, by default None.\n",
        "        font : str, optional\n",
        "            The font type of the text label for each detected object, by default 'Arial.ttf'.\n",
        "        pil : bool, optional\n",
        "            Whether to return a PIL Image object, by default False.\n",
        "        example : str, optional\n",
        "            A string to display on the example bounding box, by default 'abc'.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        numpy.ndarray or PIL Image\n",
        "            The original input image with the detected objects displayed as bounding boxes.\n",
        "            If `pil=True`, a PIL Image object is returned instead.\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        ValueError\n",
        "            If the input image has not been detected by calling the `predict_img()` method first.\n",
        "        \"\"\"\n",
        "        # Check if the `predict_img()` method has been called before displaying the detected objects\n",
        "        if self.results is None:\n",
        "            raise ValueError('No detected objects to display. Call predict_img() method first.')\n",
        "\n",
        "        # Call the plot() method of the `self.results` object to display the detected objects on the original image\n",
        "        display_img = self.results.plot(show_conf, line_width, font_size, font, pil, example)\n",
        "\n",
        "        # Return the displayed image\n",
        "        return display_img\n",
        "\n",
        "\n",
        "\n",
        "    def custom_display(self, colors, show_cls = True, show_conf = True):\n",
        "        \"\"\"\n",
        "        Custom display method that draws bounding boxes and labels on the original image,\n",
        "        with additional options for showing class and confidence information.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        colors : list\n",
        "            A list of tuples specifying the color of each class.\n",
        "        show_cls : bool, optional\n",
        "            Whether to show class information in the label text. Default is True.\n",
        "        show_conf : bool, optional\n",
        "            Whether to show confidence information in the label text. Default is True.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        numpy.ndarray\n",
        "            The image with bounding boxes and labels drawn on it.\n",
        "        \"\"\"\n",
        "\n",
        "        img = self.orig_img\n",
        "        # calculate the bounding box thickness based on the image width and height\n",
        "        bbx_thickness = (img.shape[0] + img.shape[1]) // 450\n",
        "\n",
        "        for box in self.results.boxes:\n",
        "            textString = \"\"\n",
        "\n",
        "            # Extract object class and confidence score\n",
        "            score = box.conf.item() * 100\n",
        "            class_id = int(box.cls.item())\n",
        "\n",
        "            x1 , y1 , x2, y2 = np.squeeze(box.xyxy.numpy()).astype(int)\n",
        "\n",
        "            # Print detection info\n",
        "            if show_cls:\n",
        "                textString += f\"{self.labels[class_id]}\"\n",
        "\n",
        "            if show_conf:\n",
        "                textString += f\" {score:,.2f}%\"\n",
        "\n",
        "            # Calculate font scale based on object size\n",
        "            font = cv2.FONT_HERSHEY_COMPLEX\n",
        "            fontScale = (((x2 - x1) / img.shape[0]) + ((y2 - y1) / img.shape[1])) / 2 * 2.5\n",
        "            fontThickness = 1\n",
        "            textSize, baseline = cv2.getTextSize(textString, font, fontScale, fontThickness)\n",
        "\n",
        "            # Draw bounding box, a centroid and label on the image\n",
        "            img = cv2.rectangle(img, (x1,y1), (x2,y2), colors[class_id], bbx_thickness)\n",
        "            center_coordinates = ((x1 + x2)//2, (y1 + y2) // 2)\n",
        "\n",
        "            img =  cv2.circle(img, center_coordinates, 5 , (0,0,255), -1)\n",
        "\n",
        "             # If there are no details to show on the image\n",
        "            if textString != \"\":\n",
        "                if (y1 < textSize[1]):\n",
        "                    y1 = y1 + textSize[1]\n",
        "                else:\n",
        "                    y1 -= 2\n",
        "                # show the details text in a filled rectangle\n",
        "                img = cv2.rectangle(img, (x1, y1), (x1 + textSize[0] , y1 -  textSize[1]), colors[class_id], cv2.FILLED)\n",
        "                img = cv2.putText(img, textString ,\n",
        "                    (x1, y1), font,\n",
        "                    fontScale,  (0, 0, 0), fontThickness, cv2.LINE_AA)\n",
        "\n",
        "        return img\n",
        "\n",
        "\n",
        "    def predict_video(self, video_path, save_dir, save_format=\"avi\", display='custom', verbose=True, **display_args):\n",
        "        \"\"\"Runs object detection on each frame of a video and saves the output to a new video file.\n",
        "\n",
        "        Args:\n",
        "        ----------\n",
        "            video_path (str): The path to the input video file.\n",
        "            save_dir (str): The path to the directory where the output video file will be saved.\n",
        "            save_format (str, optional): The format for the output video file. Defaults to \"avi\".\n",
        "            display (str, optional): The type of display for the detection results. Defaults to 'custom'.\n",
        "            verbose (bool, optional): Whether to print information about the video file and output file. Defaults to True.\n",
        "            **display_args: Additional arguments to be passed to the display function.\n",
        "\n",
        "        Returns:\n",
        "        ------------\n",
        "            None\n",
        "        \"\"\"\n",
        "        # Open the input video file\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "        # Get the name of the input video file\n",
        "        vid_name = os.path.basename(video_path)\n",
        "\n",
        "        # Get the dimensions of each frame in the input video file\n",
        "        width = int(cap.get(3))  # get `width`\n",
        "        height = int(cap.get(4))  # get `height`\n",
        "\n",
        "        # Create the directory for the output video file if it does not already exist\n",
        "        if not os.path.isdir(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "\n",
        "        # Set the name and path for the output video file\n",
        "        save_name = self.model_name + ' -- ' + vid_name.split('.')[0] + '.' + save_format\n",
        "        save_file = os.path.join(save_dir, save_name)\n",
        "\n",
        "        # Print information about the input and output video files if verbose is True\n",
        "        if verbose:\n",
        "            print(\"----------------------------\")\n",
        "            print(f\"DETECTING OBJECTS IN : {vid_name} : \")\n",
        "            print(f\"RESOLUTION : {width}x{height}\")\n",
        "            print('SAVING TO :' + save_file)\n",
        "\n",
        "        # Define an output VideoWriter object\n",
        "        out = cv2.VideoWriter(save_file,\n",
        "                              cv2.VideoWriter_fourcc(*\"MJPG\"),\n",
        "                              30, (width, height))\n",
        "\n",
        "        # Check if the input video file was opened correctly\n",
        "        if not cap.isOpened():\n",
        "            print(\"Error opening video stream or file\")\n",
        "\n",
        "        # Read each frame of the input video file\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "\n",
        "            # If the frame was not read successfully, break the loop\n",
        "            if not ret:\n",
        "                print(\"Error reading frame\")\n",
        "                break\n",
        "\n",
        "            # Run object detection on the frame and calculate FPS\n",
        "            beg = time.time()\n",
        "            results = self.predict_img(frame, verbose=False)\n",
        "            if results is None:\n",
        "                print('***********************************************')\n",
        "            fps = 1 / (time.time() - beg)\n",
        "\n",
        "            # Display the detection results\n",
        "            if display == 'default':\n",
        "                frame = self.default_display(**display_args)\n",
        "            elif display == 'custom':\n",
        "                frame == self.custom_display(**display_args)\n",
        "\n",
        "            # Display the FPS on the frame\n",
        "            frame = cv2.putText(frame, f\"FPS : {fps:,.2f}\",\n",
        "                                (5, 15), cv2.FONT_HERSHEY_COMPLEX,\n",
        "                                0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
        "\n",
        "            # Write the frame to the output video file\n",
        "            out.write(frame)\n",
        "\n",
        "            # Exit the loop if the 'q' button is pressed\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break\n",
        "\n",
        "        # After the loop release the cap and video writer\n",
        "        cap.release()\n",
        "        out.release()\n",
        "\n",
        "class YOLOv8_ObjectCounter(YOLOv8_ObjectDetector):\n",
        "    \"\"\"\n",
        "    A class for counting objects in images or videos using the YOLOv8 Object Detection model.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    model_file : str\n",
        "        The filename of the YOLOv8 object detection model.\n",
        "    labels : list or None\n",
        "        The list of labels for the object detection model. If None, the labels will be loaded from the model file.\n",
        "    classes : list or None\n",
        "        The list of classes to detect. If None, all classes will be detected.\n",
        "    conf : float\n",
        "        The confidence threshold for object detection.\n",
        "    iou : float\n",
        "        The Intersection over Union (IoU) threshold for object detection.\n",
        "    track_max_age : int\n",
        "        The maximum age (in frames) of a track before it is deleted.\n",
        "    track_min_hits : int\n",
        "        The minimum number of hits required for a track to be considered valid.\n",
        "    track_iou_threshold : float\n",
        "        The IoU threshold for matching detections to existing tracks.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    predict_img(img, verbose=True)\n",
        "        Predicts objects in a single image and counts them.\n",
        "    predict_video(video_path, save_dir, save_format=\"avi\", display='custom', verbose=True, **display_args)\n",
        "        Predicts objects in a video and counts them.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_file = 'yolov8n.pt', labels= None, classes = None, conf = 0.25, iou = 0.45,\n",
        "                 track_max_age = 50, track_min_hits= 10, track_iou_threshold = 0.3 ):\n",
        "\n",
        "        super().__init__(model_file , labels, classes, conf, iou)\n",
        "\n",
        "        self.track_max_age = track_max_age\n",
        "        self.track_min_hits = track_min_hits\n",
        "        self.track_iou_threshold = track_iou_threshold\n",
        "        self.class_id_arr = []\n",
        "        self.result_id_arr = []\n",
        "        # self.drop_list = []\n",
        "        self.track_frame_arr = []\n",
        "        self.distance_arr_x = []\n",
        "        self.distance_arr_y = []\n",
        "        self.personCount = []\n",
        "    def predict_video(self, video_path, save_dir, save_format = \"avi\",\n",
        "                      display = 'custom', verbose = True, **display_args):\n",
        "\n",
        "        \"\"\"\n",
        "    Runs object detection on a video file and saves the output as a new video file.\n",
        "\n",
        "    Args:\n",
        "        video_path (str): Path to the input video file.\n",
        "        save_dir (str): Path to the directory where the output video file will be saved.\n",
        "        save_format (str, optional): Format of the output video file. Defaults to \"avi\".\n",
        "        display (str, optional): Type of display to use for object detection results. Options are \"default\" or \"custom\".\n",
        "                                Defaults to \"custom\".\n",
        "        verbose (bool, optional): If True, prints information about the input and output video files. Defaults to True.\n",
        "        **display_args (dict, optional): Additional arguments to pass to the display function.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "        \"\"\"\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        # Get video name\n",
        "        vid_name = os.path.basename(video_path)\n",
        "\n",
        "\n",
        "        # Get frame dimensions and print information about input video file\n",
        "        width  = int(cap.get(3) )  # get `width`\n",
        "        height = int(cap.get(4) )  # get `height`\n",
        "\n",
        "        if not os.path.isdir(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "\n",
        "        save_name = self.model_name + ' -- ' + vid_name.split('.')[0] + '.' + save_format\n",
        "        save_file = os.path.join(save_dir, save_name)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"----------------------------\")\n",
        "            print(f\"DETECTING OBJECTS IN : {vid_name} : \")\n",
        "            print(f\"RESOLUTION : {width}x{height}\")\n",
        "            print('SAVING TO :' + save_file)\n",
        "\n",
        "        # define an output VideoWriter  object\n",
        "        out = cv2.VideoWriter(save_file,\n",
        "                            cv2.VideoWriter_fourcc(*\"MJPG\"),\n",
        "                            30,(width,height))\n",
        "\n",
        "        # Check if the video is opened correctly\n",
        "        if not cap.isOpened():\n",
        "            print(\"Error opening video stream or file\")\n",
        "\n",
        "        # Initialize object tracker\n",
        "        tracker = Sort(max_age = self.track_max_age, min_hits= self.track_min_hits ,\n",
        "                            iou_threshold = self.track_iou_threshold)\n",
        "\n",
        "        # Initialize variables for object counting\n",
        "        totalCount = []\n",
        "        personCount =  []\n",
        "        carCount = []\n",
        "        currentArray = np.empty((0, 6))\n",
        "        class_id_arr = []\n",
        "        result_id_arr = []\n",
        "\n",
        "        track_frame_arr = np.zeros(1000)\n",
        "        distance_arr_x = np.zeros(1000)\n",
        "        distance_arr_y = np.zeros(1000)\n",
        "        past_value_arr_x = np.zeros(1000)\n",
        "        past_value_arr_y = np.zeros(1000)\n",
        "        # Read the video frames\n",
        "        while cap.isOpened():\n",
        "\n",
        "            detections = np.empty((0, 6))\n",
        "            ret, frame = cap.read()\n",
        "\n",
        "            # If the frame was not read successfully, break the loop\n",
        "            if not ret:\n",
        "                print(\"Error reading frame\")\n",
        "                break\n",
        "\n",
        "            # Run object detection on the frame and calculate FPS\n",
        "            beg = time.time()\n",
        "            results = self.predict_img(frame, verbose = False)\n",
        "            if results == None:\n",
        "                print('***********************************************')\n",
        "            fps = 1 / (time.time() - beg)\n",
        "            for box in results.boxes:\n",
        "                score = box.conf.item() * 100\n",
        "                class_id = int(box.cls.item())#------------------------------------------------------------------------------------------------------------\n",
        "                # print(\"class_id: \", class_id)\n",
        "\n",
        "                # print(\"class_id_arr: \", class_id_arr)\n",
        "\n",
        "                x1 , y1 , x2, y2 = np.squeeze(box.xyxy.numpy()).astype(int)\n",
        "\n",
        "                currentArray = np.array([x1, y1, x2, y2, score, class_id])\n",
        "                detections = np.vstack((detections, currentArray))\n",
        "\n",
        "                # # Count the number of each object type\n",
        "                # if class_id in object_count:\n",
        "                #     object_count[class_id] += 1\n",
        "                # else:\n",
        "                #     object_count[class_id] = 1\n",
        "\n",
        "\n",
        "            # Update object tracker\n",
        "            resultsTracker = tracker.update(detections)\n",
        "            # self.drop_list.append(tracker.drop_list) -----------------------------DROP LIST\n",
        "            # print(self.drop_list)\n",
        "            for result in resultsTracker:\n",
        "                # print(\"result shape:\",result.shape)\n",
        "\n",
        "                # Get the tracker results\n",
        "                x1, y1, x2, y2, id, class_id = result\n",
        "                x1, y1, x2, y2, id, class_id = int(x1), int(y1), int(x2), int(y2), int(id), int(class_id)\n",
        "                # print(\"result:\",result)----------------------------------------------------------------------\n",
        "                # result_id_arr.append(id)\n",
        "                # class_id_arr.append(class_id)\n",
        "                # print(\"result_id_arr: \", result_id_arr)\n",
        "                # print(\"class_id_arr: \", class_id_arr)\n",
        "\n",
        "                # if id < 10:\n",
        "                #   print(\"result:\",result)\n",
        "                # if id == 1:\n",
        "                #   id1_arr.append(y1)\n",
        "                #   print(\"id1_y1:\",id1_arr)\n",
        "                # if id == 3:\n",
        "                #   id3_arr.append(y1)\n",
        "                #   print(\"id3_y1:\",id3_arr)\n",
        "                # if id == 2:\n",
        "                #   id2_arr.append(y1)\n",
        "                #   print(\"id2_y1:\",id2_arr)\n",
        "                # if id == 4:\n",
        "                #   id4_arr.append(y1)\n",
        "                #   print(\"id4_y1:\",id4_arr)\n",
        "                # if id == 6:\n",
        "                #   id6_arr.append(y1)\n",
        "                #   print(\"id6_y1:\",id6_arr)\n",
        "                # if id == 9:\n",
        "                #   id9_arr.append(y1)\n",
        "                #   print(\"id9_y1:\",id9_arr)\n",
        "                # if id == 24:\n",
        "                #   id9_arr.append(y1)\n",
        "                #   print(\"id24_y1:\",id24_arr)\n",
        "\n",
        "                # Calculate moving distance using y1:\n",
        "\n",
        "\n",
        "                # Display current objects IDs\n",
        "                w, h = x2 - x1, y2 - y1\n",
        "                cx, cy = x1 + w // 2, y1 + h // 2\n",
        "                id_txt = f\"ID: {str(id)}\"\n",
        "                cv2.putText(frame, id_txt, (cx, cy), 4, 0.5, (0, 0, 255), 1)\n",
        "\n",
        "                # if we haven't seen aprticular object ID before, register it in a list\n",
        "                if totalCount.count(id) == 0:\n",
        "                    totalCount.append(id)\n",
        "                    result_id_arr.append(id)\n",
        "                    class_id_arr.append(class_id)\n",
        "\n",
        "                track_frame_arr[id] += 1\n",
        "                if past_value_arr_y[id] == 0:\n",
        "                  past_value_arr_y[id] = cy\n",
        "                else:\n",
        "                  distance_arr_y[id] += past_value_arr_y[id] - cy\n",
        "\n",
        "                if past_value_arr_x[id] == 0:\n",
        "                  past_value_arr_x[id] = cx\n",
        "                else:\n",
        "                   distance_arr_x[id] += past_value_arr_x[id] - cx\n",
        "                # for result_id, class_id in zip(result_id_arr, class_id_arr):\n",
        "                #   if class_id == 0:\n",
        "                #     personCount.append(id)\n",
        "\n",
        "                # for result_id, class_id in zip(result_id_arr, class_id_arr):\n",
        "                #   if class_id == 2:\n",
        "                #     carCount.append(result_id)\n",
        "                    # print(\"class len:\", len(class_id_arr))\n",
        "                # for result_id, class_id in zip(result_id_arr, class_id_arr):\n",
        "                if personCount.count(id) == 0 and class_id == 0:\n",
        "                  personCount.append(id)\n",
        "\n",
        "                # for result_id, class_id in zip(result_id_arr, class_id_arr):\n",
        "                if carCount.count(id) == 0 and class_id == 2:\n",
        "                  carCount.append(id)\n",
        "            #make sure all elements in class_id_arr and result_id_arr matched\n",
        "            # n = len(class_id_arr)-len(result_id_arr)\n",
        "            # if(n!=0):\n",
        "            #     del class_id_arr[-n:]\n",
        "            # print(\"class_id_length: \", len(class_id_arr))\n",
        "            # print(\"result_id_length: \", len(result_id_arr))\n",
        "\n",
        "\n",
        "            # Display detection results\n",
        "            if display == 'default':\n",
        "                frame = self.default_display(**display_args)\n",
        "\n",
        "            elif display == 'custom':\n",
        "                frame == self.custom_display( **display_args)\n",
        "\n",
        "            # Display FPS on frame\n",
        "            frame = cv2.putText(frame,f\"FPS : {fps:,.2f}\" ,\n",
        "                                (5,55), cv2.FONT_HERSHEY_COMPLEX,\n",
        "                            0.5,  (0,255,255), 1, cv2.LINE_AA)\n",
        "\n",
        "            # Display Counting results\n",
        "            count_txt = f\"TOTAL COUNT : {len(totalCount)}\"\n",
        "            frame = cv2.putText(frame, count_txt, (5,45), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 0, 255), 2)\n",
        "            # print(totalCount)\n",
        "\n",
        "            # Display Counting results\n",
        "            count_txt = f\"PERSON COUNT : {len(personCount)}\"\n",
        "            frame = cv2.putText(frame, count_txt, (5,75), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 0, 255), 2)\n",
        "            # print(personCount)\n",
        "\n",
        "            # Display Counting results\n",
        "            count_txt = f\"CAR COUNT : {len(carCount)}\"\n",
        "            frame = cv2.putText(frame, count_txt, (5,115), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 0, 255), 2)\n",
        "            # print(carCount)\n",
        "\n",
        "            # append frame to the video file\n",
        "            out.write(frame)\n",
        "\n",
        "            # the 'q' button is set as the\n",
        "            # quitting button you may use any\n",
        "            # desired button of your choice\n",
        "\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break\n",
        "\n",
        "        # After the loop release the cap\n",
        "        cap.release()\n",
        "        out.release()\n",
        "        print(len(totalCount))\n",
        "        print(totalCount)\n",
        "        # print(\"mean_id1\", np.mean(id1_arr))\n",
        "        # print(\"mean_id3\", np.mean(id3_arr))\n",
        "        # print(\"mean_id2\", np.mean(id2_arr))\n",
        "        # # print(\"mean_id4\", np.mean(id4_arr))\n",
        "        # print(\"mean_id6\", np.mean(id6_arr))\n",
        "        # print(\"mean_id9\", np.mean(id9_arr))\n",
        "        # print(\"mean_id24\", np.mean(id24_arr))\n",
        "        print(\"class_id_arr: \", class_id_arr)\n",
        "        print(\"result_id_arr: \", result_id_arr)\n",
        "        # print(\"class_id_length: \", len(class_id_arr))\n",
        "        # print(\"result_id_length: \", len(result_id_arr))\n",
        "        self.class_id_arr = class_id_arr\n",
        "        self.result_id_arr = result_id_arr\n",
        "        self.track_frame_arr = track_frame_arr\n",
        "        self.distance_arr_x = distance_arr_x\n",
        "        self.distance_arr_y = distance_arr_y\n",
        "        self.personCount = personCount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5vIF4KCKudC"
      },
      "outputs": [],
      "source": [
        "vid_results_path = '/content/video_object_detection_results'\n",
        "test_vids_path = '/content/Object-tracking-and-counting-using-YOLOV8/test vids'\n",
        "\n",
        "\n",
        "if not os.path.isdir(vid_results_path):\n",
        "    os.makedirs(vid_results_path)\n",
        "\n",
        "if not os.path.isdir(test_vids_path):\n",
        "    os.makedirs(test_vids_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "976BVVQ3LHIY"
      },
      "outputs": [],
      "source": [
        "# yolo_names = ['yolov8n.pt', 'yolov8m.pt', 'yolov8s.pt',  'yolov8l.pt']\n",
        "yolo_names = ['yolov8m.pt']\n",
        "colors = []\n",
        "for _ in range(80):\n",
        "    rand_tuple = (random.randint(50, 255), random.randint(50, 255), random.randint(50, 255))\n",
        "    colors.append(rand_tuple)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLRd9e2_LIow",
        "outputId": "b9679d34-9b3f-48dc-d9e9-2ca2f80f6e73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m.pt to 'yolov8m.pt'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49.7M/49.7M [00:00<00:00, 168MB/s]\n"
          ]
        }
      ],
      "source": [
        "counters = []\n",
        "for yolo_name in yolo_names:\n",
        "    counter = YOLOv8_ObjectCounter(yolo_name, conf = 0.60 )\n",
        "    counters.append(counter)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snL96boWLLdQ",
        "outputId": "0e4f1d11-511d-42a9-91ab-b0ad675ae022"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------\n",
            "DETECTING OBJECTS IN : st-catherines_drive.mp4 : \n",
            "RESOLUTION : 2562x1440\n",
            "SAVING TO :/content/video_object_detection_results/yolov8m -- st-catherines_drive.avi\n",
            "Error reading frame\n",
            "135\n",
            "[10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 11, 12, 13, 17, 14, 15, 20, 24, 26, 31, 36, 38, 28, 40, 39, 30, 45, 47, 56, 59, 65, 66, 61, 68, 73, 77, 78, 80, 81, 95, 96, 103, 97, 84, 112, 123, 120, 125, 113, 108, 132, 137, 133, 141, 140, 135, 129, 142, 146, 144, 150, 152, 148, 153, 161, 160, 159, 166, 175, 176, 181, 182, 184, 185, 186, 178, 188, 187, 189, 206, 211, 208, 212, 213, 216, 217, 222, 225, 226, 240, 230, 238, 249, 254, 265, 272, 274, 279, 285, 294, 300, 303, 308, 320, 342, 340, 344, 346, 348, 353, 351, 368, 369, 380, 377, 384, 385, 389, 392, 398, 395, 402, 393, 390, 408, 403, 412, 410, 415, 422, 428, 429, 430, 432, 434]\n",
            "class_id_arr:  [2, 0, 2, 7, 2, 0, 0, 2, 2, 2, 0, 0, 0, 2, 1, 1, 2, 2, 0, 7, 2, 5, 0, 2, 2, 0, 2, 2, 2, 0, 2, 0, 2, 2, 2, 0, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 0, 2, 2, 0, 2, 2, 0, 2, 0, 2, 2, 2, 0, 0, 2, 2, 2, 2, 0, 0, 2, 0, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 2, 9, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 2, 1, 0, 58, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2]\n",
            "result_id_arr:  [10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 11, 12, 13, 17, 14, 15, 20, 24, 26, 31, 36, 38, 28, 40, 39, 30, 45, 47, 56, 59, 65, 66, 61, 68, 73, 77, 78, 80, 81, 95, 96, 103, 97, 84, 112, 123, 120, 125, 113, 108, 132, 137, 133, 141, 140, 135, 129, 142, 146, 144, 150, 152, 148, 153, 161, 160, 159, 166, 175, 176, 181, 182, 184, 185, 186, 178, 188, 187, 189, 206, 211, 208, 212, 213, 216, 217, 222, 225, 226, 240, 230, 238, 249, 254, 265, 272, 274, 279, 285, 294, 300, 303, 308, 320, 342, 340, 344, 346, 348, 353, 351, 368, 369, 380, 377, 384, 385, 389, 392, 398, 395, 402, 393, 390, 408, 403, 412, 410, 415, 422, 428, 429, 430, 432, 434]\n"
          ]
        }
      ],
      "source": [
        "for counter in counters:\n",
        "\n",
        "    counter.predict_video(video_path= '/content/drive/MyDrive/ECSE415/Colab_Notebooks_Final_Project/st-catherines_drive.mp4', save_dir = vid_results_path, save_format = \"avi\", display = 'custom', colors = colors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIZzYV0iSg8f",
        "outputId": "a2b50428-885b-4439-c5d8-ff06cd5e1d61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "St-Catherine: Number of parking cars:  54\n",
            "St-Catherine:Number of moving cars:  12\n",
            "St-Catherine:Number of pedestrians:  63\n"
          ]
        }
      ],
      "source": [
        "# St-Catherine\n",
        "class_id_arr = np.array(counter.class_id_arr)\n",
        "result_id_arr = np.array(counter.result_id_arr)\n",
        "distance_arr_x = np.array(counter.distance_arr_x)\n",
        "distance_arr_y = np.array(counter.distance_arr_y)\n",
        "track_frame_arr = np.array(counter.track_frame_arr)\n",
        "# personCount = np.array(counter.personCount)\n",
        "\n",
        "velocity_arr = np.zeros(1000)\n",
        "parking_num = 0\n",
        "moving_num = 0\n",
        "velocity_index = 0\n",
        "# velocity_sum = 0\n",
        "# velocity_non_zero = 0\n",
        "# velocity_mean = 0\n",
        "# velocity_non_zero = []\n",
        "# velovity_median = 0\n",
        "\n",
        "for i in range (len(distance_arr_x)):\n",
        "  if (distance_arr_x[i] != 0) and (track_frame_arr[i] != 0) and (distance_arr_y[i] != 0) :\n",
        "    velocity_arr[i] = np.sqrt(np.power(distance_arr_x[i],2) + np.power(distance_arr_y[i],2))/track_frame_arr[i]\n",
        "\n",
        "# for j in range (len(velocity_arr)):\n",
        "#   if velocity_arr[j] > 70:\n",
        "#     velocity_sum += velocity_arr[j]\n",
        "#     velocity_non_zero += 1\n",
        "# velocity_mean = velocity_sum/velocity_non_zero\n",
        "# for j in range (len(velocity_arr)):\n",
        "#   if velocity_arr[j] > 70:\n",
        "#     velocity_non_zero.append(velocity_arr[j])\n",
        "# velocity_median = np.median(velocity_non_zero, axis = None, out = None)\n",
        "\n",
        "for index in range (len(result_id_arr)):\n",
        "  if class_id_arr[index] == 2:\n",
        "    velocity_index = result_id_arr[index]\n",
        "    if velocity_arr[velocity_index] < 400 and velocity_arr[velocity_index] > 70:\n",
        "      parking_num += 1\n",
        "    else:\n",
        "      moving_num += 1\n",
        "# print(velocity_median)\n",
        "# print(\"velocity:\", velocity_arr)\n",
        "# print(\"x: \", distance_arr_x)\n",
        "# print(\"y: \",distance_arr_y)\n",
        "# print(\"frame: \",track_frame_arr)\n",
        "print(\"St-Catherine: Number of parking cars: \", parking_num)\n",
        "print(\"St-Catherine:Number of moving cars: \", moving_num)\n",
        "print(\"St-Catherine:Number of pedestrians: \", len(counter.personCount))\n",
        "# print(\"Number of pedestrians: \", len(personCount))\n",
        "# print(track_frame_arr)\n",
        "#print(counter.track_frame_arr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rk__MVHyLI1G"
      },
      "outputs": [],
      "source": [
        "counters2 = []\n",
        "for yolo_name in yolo_names:\n",
        "    counter2 = YOLOv8_ObjectCounter(yolo_name, conf = 0.60 )\n",
        "    counters2.append(counter2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwNrVbh_LN-c",
        "outputId": "76bad713-7209-4a3d-9102-377fd6814238"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------\n",
            "DETECTING OBJECTS IN : mcgill_drive.mp4 : \n",
            "RESOLUTION : 2562x1440\n",
            "SAVING TO :/content/video_object_detection_results/yolov8m -- mcgill_drive.avi\n",
            "Error reading frame\n",
            "81\n",
            "[439, 438, 437, 436, 440, 441, 444, 447, 446, 450, 452, 459, 471, 467, 472, 474, 476, 479, 480, 486, 488, 501, 505, 506, 510, 507, 512, 511, 513, 508, 515, 519, 520, 528, 545, 541, 549, 554, 557, 558, 564, 561, 567, 576, 586, 597, 595, 596, 604, 603, 610, 614, 617, 622, 623, 638, 634, 640, 632, 642, 639, 650, 654, 655, 660, 663, 667, 681, 674, 661, 687, 694, 698, 702, 713, 712, 719, 726, 727, 728, 729]\n",
            "class_id_arr:  [2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 0, 0, 2, 2, 2, 9, 1, 0, 0, 2, 7, 0, 0, 2, 2, 2, 2, 3, 2, 10, 9, 9, 0, 2, 1, 2, 1, 1, 0, 2, 2, 2, 2, 9, 2, 2, 2, 2, 0, 2, 0, 1, 2, 10, 2, 0, 2, 2, 2, 2, 2, 2, 9, 9, 0, 2, 9, 2, 0, 2, 2, 5, 0, 2, 2, 2, 2, 2]\n",
            "result_id_arr:  [439, 438, 437, 436, 440, 441, 444, 447, 446, 450, 452, 459, 471, 467, 472, 474, 476, 479, 480, 486, 488, 501, 505, 506, 510, 507, 512, 511, 513, 508, 515, 519, 520, 528, 545, 541, 549, 554, 557, 558, 564, 561, 567, 576, 586, 597, 595, 596, 604, 603, 610, 614, 617, 622, 623, 638, 634, 640, 632, 642, 639, 650, 654, 655, 660, 663, 667, 681, 674, 661, 687, 694, 698, 702, 713, 712, 719, 726, 727, 728, 729]\n"
          ]
        }
      ],
      "source": [
        "for counter2 in counters2:\n",
        "\n",
        "    counter2.predict_video(video_path= '/content/drive/MyDrive/ECSE415/Colab_Notebooks_Final_Project/mcgill_drive.mp4', save_dir = vid_results_path, save_format = \"avi\", display = 'custom', colors = colors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YXOD_n0cMA3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32f6a05f-05e1-4f06-b364-28d1341f7cd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "McGill: Number of parking cars:  14\n",
            "McGill:Number of moving cars:  34\n",
            "McGill:Number of pedestrians:  16\n"
          ]
        }
      ],
      "source": [
        "# McGill\n",
        "class_id_arr = np.array(counter2.class_id_arr)\n",
        "result_id_arr = np.array(counter2.result_id_arr)\n",
        "distance_arr_x = np.array(counter2.distance_arr_x)\n",
        "distance_arr_y = np.array(counter2.distance_arr_y)\n",
        "track_frame_arr = np.array(counter2.track_frame_arr)\n",
        "# personCount = np.array(counter2.personCount)\n",
        "\n",
        "velocity_arr = np.zeros(1000)\n",
        "parking_num = 0\n",
        "moving_num = 0\n",
        "velocity_index = 0\n",
        "# velocity_sum = 0\n",
        "# velocity_non_zero = 0\n",
        "# velocity_mean = 0\n",
        "# velocity_non_zero = []\n",
        "# velovity_median = 0\n",
        "\n",
        "for i in range (len(distance_arr_x)):\n",
        "  if (distance_arr_x[i] != 0) and (track_frame_arr[i] != 0) and (distance_arr_y[i] != 0) :\n",
        "    velocity_arr[i] = np.sqrt(np.power(distance_arr_x[i],2) + np.power(distance_arr_y[i],2))/track_frame_arr[i]\n",
        "\n",
        "# for j in range (len(velocity_arr)):\n",
        "#   if velocity_arr[j] > 70:\n",
        "#     velocity_sum += velocity_arr[j]\n",
        "#     velocity_non_zero += 1\n",
        "# velocity_mean = velocity_sum/velocity_non_zero\n",
        "# for j in range (len(velocity_arr)):\n",
        "#   if velocity_arr[j] > 70:\n",
        "#     velocity_non_zero.append(velocity_arr[j])\n",
        "# velocity_median = np.median(velocity_non_zero, axis = None, out = None)\n",
        "\n",
        "for index in range (len(result_id_arr)):\n",
        "  if class_id_arr[index] == 2:\n",
        "    velocity_index = result_id_arr[index]\n",
        "    if velocity_arr[velocity_index] < 200 and velocity_arr[velocity_index] > 70:\n",
        "      parking_num += 1\n",
        "    else:\n",
        "      moving_num += 1\n",
        "# print(velocity_median)\n",
        "# print(\"velocity:\", velocity_arr)\n",
        "# print(\"x: \", distance_arr_x)\n",
        "# print(\"y: \",distance_arr_y)\n",
        "# print(\"frame: \",track_frame_arr)\n",
        "print(\"McGill: Number of parking cars: \", parking_num)\n",
        "print(\"McGill:Number of moving cars: \", moving_num)\n",
        "print(\"McGill:Number of pedestrians: \", len(counter2.personCount))\n",
        "# print(\"Number of pedestrians: \", len(personCount))\n",
        "# print(track_frame_arr)\n",
        "#print(counter2.track_frame_arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XClAoTi3MaDp"
      },
      "source": [
        "# **Report**\n",
        "\n",
        "\n",
        "1.   **Description of the overall approach**\n",
        "\n",
        "      The objective of this project involves detecting and enumerating objects in two short video sequences while distinguishing between stationary and moving cars within the scenes. The team opted to utilize YOLOv8 as the foundation of our model due to its rapid object detection and classification capabilities per frame. Following detection in each frame, the output undergoes analysis through the Simple Online and Real-time Tracking (SORT) function, verifying the validity of detections for subsequent tracking and assigning tracking IDs. By tallying the unique detections, the team conducts object counting and tracking. To differentiate between stationary and moving cars, an additional function calculates the average velocity of each identified object, enabling a comparison. Higher velocities indicate moving cars, while lower velocities signify stationary cars. The program concludes by displaying the count for each classification.\n",
        "\n",
        "2.   **Software Package and Routine Used**\n",
        "* YOLOv8 object detection and classification:\n",
        "\n",
        "  represents an advanced object detection and classification model known for its speed and accuracy. As an evolution in the YOLO series, this model employs a single neural network to simultaneously predict bounding boxes and class probabilities for objects within an image. YOLOv8 operates by dividing the input image into a grid and generating bounding boxes along with class predictions directly from this grid. Notably, its architecture enables real-time detection of objects within images or video frames.\n",
        "\n",
        "* SORT:\n",
        "\n",
        "  The Simple Online and Real-time Tracking (SORT) algorithm is a simple and efficient algorithm that can track multiple objects simultaneously. It relies on object detection bounding boxes and implements a data association technique to match detections with existing tracks based on proximity. SORT utilizes the Kalman filter for prediction and track management, adapting to occlusions and handling object disappearances or reappearances. SORT provides reliable and real-time tracking capabilities and it is suitable under the context of this project.\n",
        "\n",
        "3. **Summary of Program Output**\n",
        "\n",
        "* McGill:\n",
        "\n",
        "  * Ground Truth Value (manually counted):\n",
        "\n",
        "\tMoving car: 27\n",
        "\n",
        "\tParking car: 15\n",
        "\n",
        "    Person: 30\n",
        "\n",
        " * Program Output Value:\n",
        "\n",
        "\tMoving car: 35\n",
        "\n",
        "\tParking car: 14\n",
        "\n",
        "   Person: 16\n",
        "\n",
        "* St. Catherine:\n",
        "\n",
        " * Ground Truth Value (manually counted):\n",
        "\n",
        "\tMoving car: 10\n",
        "\n",
        "\tParking car: 57\n",
        "\n",
        "\tPerson: 73\n",
        "\n",
        " * Program Output Value:\n",
        "\n",
        "\tMoving car: 12\n",
        "\n",
        "\tParking car: 54\n",
        "\n",
        "    Person: 63\n",
        "\n",
        "4. **Program Performance and Problems**\n",
        "* Counting Accuracy:\n",
        "\n",
        " * Mcgill:\n",
        "\n",
        "    moving cars: 70.37%\n",
        "\n",
        "    parking cars: 93.33%\n",
        "\n",
        "    pedestrians: 53.33%\n",
        "\n",
        " * St-Catherine:\n",
        "\n",
        "    moving cars: 80.00%\n",
        "\n",
        "    parking cars: 94.73%\n",
        "\n",
        "    pedestrians: 86.3%\n",
        "\n",
        "In comparison to the manually counted values for both videos, we have observed a satisfactory accuracy in detecting parking cars. While this result is promising, our program still faces challenges in accurately determining the status of each car. There are two main reasons for this. Firstly, the detection system tends to identify moving trucks as moving cars due to their similar shapes. This leads to the system detecting additional moving vehicles, causing the count to be higher than what was manually observed. Secondly, both videos are captured using a dash camera. The simultaneous movement of the camera and the observed cars introduces complexities in selecting an appropriate reference velocity. This makes it difficult to accurately determine the status of vehicles by comparing their velocities.\n",
        "Additionally, there is a considerable loss in the count of pedestrians. From the result output video, it is obvious that certain pedestrians go undetected, resulting in an omission from the overall count. This detection issue is especially noticeable for pedestrians wearing dark clothes or walking under trees. When there isn't enough light on an object, the detection system experiences failures, consequently contributing to a loss in the accurate counting of pedestrians.\n",
        "It's worth noting that our program performs better overall on videos captured on St-Catherine. This is because St-Catherine has brighter lighting compared to McGill, which enhances the detection of pedestrians. Additionally, St-Catherine is a one-way street, with cars moving in the same direction, making it easier to select a more suitable reference velocity for accurately detecting the status of cars.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "f7VLVQ5kSh7G"
      },
      "outputs": [],
      "source": [
        "from pycocotools.coco import COCO\n",
        "\n",
        "with open('/content/Object-tracking-and-counting-using-YOLOV8/coco.names') as f:\n",
        "  cats = f.read().splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "YO7vBPA5Sl6E"
      },
      "outputs": [],
      "source": [
        "totalcount = np.unique(result_id_arr)\n",
        "names_arr = []\n",
        "for id_val in totalcount:\n",
        "  index_object = np.where(result_id_arr == id_val)[0] # Using np.where to find index\n",
        "  if len(index_object) > 0: # Check if the ID is found\n",
        "    index_object = index_object[0] # Select the first index if multiple found\n",
        "    id_class = class_id_arr[index_object]\n",
        "    name = cats[id_class]\n",
        "    names_arr.append(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXH1LLxuTESM",
        "outputId": "fabdd3f8-2d59-4a8e-ef6e-63bf86fab6e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total count - Mcgill: 81\n",
            "bicycle : 5\n",
            "bus : 1\n",
            "car : 48\n",
            "fire hydrant : 2\n",
            "motorbike : 1\n",
            "person : 16\n",
            "traffic light : 7\n",
            "truck : 1\n"
          ]
        }
      ],
      "source": [
        "print(\"total count - Mcgill:\",len(names_arr))\n",
        "name_unique = np.unique(names_arr)\n",
        "for category in name_unique:\n",
        "  print(category,\":\", names_arr.count(category))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "totalcount = np.unique(counter.result_id_arr)\n",
        "names_arr = []\n",
        "for id_val in totalcount:\n",
        "  index_object = np.where(counter.result_id_arr == id_val)[0] # Using np.where to find index\n",
        "  if len(index_object) > 0: # Check if the ID is found\n",
        "    index_object = index_object[0] # Select the first index if multiple found\n",
        "    id_class = counter.class_id_arr[index_object]\n",
        "    name = cats[id_class]\n",
        "    names_arr.append(name)"
      ],
      "metadata": {
        "id": "m9qFattGhgrm"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"total count - St_Catherine:\",len(names_arr))\n",
        "name_unique = np.unique(names_arr)\n",
        "for category in name_unique:\n",
        "  print(category,\":\", names_arr.count(category))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdu7CDSIhoV5",
        "outputId": "de66b39f-942d-4bd7-a39d-a241f0b01520"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total count - St_Catherine: 135\n",
            "bicycle : 5\n",
            "bus : 1\n",
            "car : 66\n",
            "person : 58\n",
            "pottedplant : 1\n",
            "traffic light : 2\n",
            "truck : 2\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}